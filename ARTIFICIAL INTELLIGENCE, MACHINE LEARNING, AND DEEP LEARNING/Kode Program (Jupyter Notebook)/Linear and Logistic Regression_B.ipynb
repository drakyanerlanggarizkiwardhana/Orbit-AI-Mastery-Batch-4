{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4\n",
    "## Section: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2: Advanced Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcomings of linear regression?\n",
    "***\n",
    "1. Main limitation of Linear Regression is the assumption of linearity between the dependent variable and the independent variables. In the real world, the data is rarely linearly separable. It assumes that there is a straight-line relationship between the dependent and independent variables which is incorrect many times.\n",
    "\n",
    "2. Prone to noise and overfitting: If the number of observations are lesser than the number of features, Linear Regression should not be used, otherwise it may lead to overfit because is starts considering noise in this scenario while building the model.\n",
    "\n",
    "3. Prone to outliers: Linear regression is very sensitive to outliers (anomalies). So, outliers should be analyzed and removed before applying Linear Regression to the dataset.\n",
    "\n",
    "4. Prone to multicollinearity: Before applying Linear regression, multicollinearity should be removed (using dimensionality reduction techniques) because it assumes that there is no relationship among independent variables.\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objective\n",
    "***\n",
    "- Polynomial Basis Function\n",
    "- Regularization(L1/L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br>\n",
    "## Objective\n",
    "***\n",
    "- Underfitting\n",
    "- Polynomial Regression - Introduction\n",
    "- Polynomial Basis Function - Mathematical Representation\n",
    "- What is a Polynomial Feature Transformer?\n",
    "- Overfitting\n",
    "- Concept of Regularization\n",
    "\t- L1 Regularization (also called as Lasso penalisation)\n",
    "\t\t- Understanding L1\n",
    "\t\t- Lasso - Implementation\n",
    "\t- L2 Regularization (also called as Ridge penalisation) \n",
    "\t\t- Understanding L2\n",
    "\t\t- Ridge - Implementation\n",
    "\t- L1 vs L2 Regularization\n",
    "\t- Ridge or Lasso ?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Till now we have learnt to fit a simple linear regression model and we have done this on the house-price prediction dataset.\n",
    "- We have predicted the Price of a House given its Area \n",
    "- However, with further analysis, these predictions can be improved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br><br><br><br><br><br>\n",
    "## Underfitting\n",
    "***\n",
    "\n",
    "Underfitting is when the the model *fails to capture the overall 'trend' of the data.* For example: The linear model will do an extremely poor job of explaining the target variable if we fitted to nonlinear data. It will *oversimplifies* the model.\n",
    "\n",
    "A model that underfits is said to have **high bias.**\n",
    "\n",
    "- Bias is the difference between predictions and truths using models trained by specific learning algorithm\n",
    "\n",
    "- Error due to bias is because we assumed that simple linear model will fit a non linear data distribution  while specifying the  algorithm\n",
    "\n",
    "- Basically, we are biased toward the data having a linear trend and we hypothesize that our target variable increases linearly EVEN THOUGH WE CAN SEE A CLEAR NON-LINEAR TREND \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br>\n",
    "### What is the issue here?\n",
    "\n",
    "A linear model is not able to fit the non-linear data. \n",
    "\n",
    "- But then can a non-linear curve do the job?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br>\n",
    "## Polynomial Regression - Introduction\n",
    "***\n",
    "\n",
    "\n",
    "* One way to tackle underfitting is through polynomial functions. Let's see the implementation of this technique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br><br><br><br><br><br>\n",
    "## Polynomial Basis Function - Mathematical Representation\n",
    "***\n",
    "\n",
    "* In many settings, such a linear relationship may not hold. For example, if we are modeling the yield of a chemical synthesis in terms of the temperature at which the synthesis takes place, we may find that the yield improves by increasing amounts for each unit increase in temperature. In this case, we might propose a quadratic model of the form\n",
    "    $$ y = \\beta_{0}+\\beta_{1}x+\\beta_{2}x^2+\\varepsilon. $$\n",
    "<br/>\n",
    "* In general, we can model the expected value of y as an nth degree polynomial, yielding the general polynomial regression model\n",
    "    $$ y = \\beta_{0}+\\beta_{1}x+\\beta_{2}x^2+\\cdots +\\beta_{n}x^n+\\varepsilon. $$\n",
    "<br/>\n",
    "the above is what is known as **polynomial basis function**  \n",
    " <br/>\n",
    "* Polynomial projection is built into Scikit-Learn, using the ``Polynomial Features`` transformer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br>\n",
    "## What is a Polynomial Feature Transformer? \n",
    "\n",
    " - This is nothing but taking the *n-th* degree of your variable\n",
    " - Below is an easy example to display what it does\n",
    "     - Say we wanted to take the 1st, 2nd and 3rd degree of the numbers 2,3 & 4. \n",
    "     - Intuitively we know that for 2 the 1st, 2nd and 3rd degrees are: 2, 4 $(2^2)$ and 8 $(2^3)$\n",
    " \n",
    " - Let's try and understand the code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  4.,  8.],\n",
       "       [ 3.,  9., 27.],\n",
       "       [ 4., 16., 64.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code to improvise polynomial regression model and increase the accuracy \n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "x = np.array([2, 3, 4])\n",
    "poly = PolynomialFeatures(3, include_bias=False)\n",
    "poly.fit_transform(x[:, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here the 3 in \"PolynomialFeatures(3, include_bias=False)\" is basically the value of degree of the polynomials up to which we want our data to be transformed to \n",
    "\n",
    "- We can see 3 values for 2,3 and 4. For example: $4^1$ (4), $4^2$ (16) and $4^3$ (64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Now, let's use the same technique to create a 7th-degree polynomial model for the non-linear data generated  earlier **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#make_pipeline() just like Pipeline() is used to perform a sequence of different transformations on a raw dataset before applying the final estimator. \n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "poly_model = make_pipeline(PolynomialFeatures(7),\n",
    "                           LinearRegression())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = np.sin(x) + 0.1 * rng.randn(50)\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "\n",
    "poly_model.fit(x[:, np.newaxis], y)\n",
    "yfit = poly_model.predict(xfit[:, np.newaxis])\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit)\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"Sin Function\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "7th-order polynomial basis function can provide an excellent fit to this non-linear data.\n",
    "\n",
    "There are many advantages of using Polynomial Regression as-\n",
    "-  Broad range of functions can be fit over model\n",
    "-  Polynomial function fits a wide range of curvature \n",
    "-  It provides a good approximation of the relationship and makes the transformation more interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br><br><br>\n",
    "## Overfitting \n",
    "***\n",
    "\n",
    "Overfitting occurs when your model follows the training dataset very rigorously i.e it gives low training error, but it may not work well on generalized or test dataset i.e. it may give a high generalization error. \n",
    "\n",
    "For example- It's like a person cramming word to word from a book while studying for his exams. If the questions in the test are straight as in the book, he will do well, else if the questions are more based on usage of multiple concepts outlined in the book he studied, and not directy stated, the person may struggle.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/bluedataconsulting/AIMasteryProgram/main/Lab_Exercises/Module4/images/underfitting_overfitting.png\" width=500 height=250><br/>\n",
    "\n",
    "<br/><br/>\n",
    "An overfitting model is said to have high variance.\n",
    "\n",
    "We will discuss about bias and variance soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br><br><br>\n",
    "## Concept of Regularization\n",
    "***\n",
    "<br/>\n",
    "Regularizations techniques are used to reduce the error by fitting a function appropriately on the given training set to avoid overfitting.\n",
    "Let's build an intuition with the help of an example \n",
    " \n",
    "\n",
    "* Let's say a parent is very cautious about the future of his children \n",
    "* He wants them to be successful in life without being strict with them. He takes a decision about how much flexibility should be given to his children during their upbringing. \n",
    "\n",
    "**Too much restriction may suppress their development of character,however; Too much flexibility may spoil them.**\n",
    "\n",
    "- He decides to overcome this situation with the idea of regularized(regularization) flexibility, which is to give enough flexibility added with some restrictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "The overfitting behaviour occurs when basis functions overlap:\n",
    "- The coefficients of adjacent basis functions grow large and cancel each other out.\n",
    "- We need to limit such spikes explicitly in the model by penalizing large values of the model parameters (the thetas of variables)\n",
    "- Such a penalty is known as regularization.\n",
    "\n",
    "It can be done through-\n",
    "-  L1 Regularization (also called as Lasso Penalization/Regression)\n",
    "-  L2 Regularization (also called as Ridge Penalization/Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br>\n",
    "## L1 Regularization (also called as Lasso penalisation)\n",
    "***\n",
    "<br />\n",
    "Involves penalising sum of absolute values (1-norms) of regression coefficients<br/>\n",
    "Here the Penalty (P) on the model will be:\n",
    "$$ P=\\alpha\\sum_{i=1}^{n}\\left | \\theta_{n} \\right | $$\n",
    "<br/>\n",
    "\n",
    "And the cost function would be:<br/>\n",
    "$$ J(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)}-y^{(i)})^2 + \\alpha\\sum_{n=1}^{N}\\left | \\theta_{n} \\right | $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br><br><br>\n",
    "### Understanding L1\n",
    "\n",
    " - Here we are familiar with the First half of the Cost Function. \n",
    " \n",
    " - By adding all thetas to the cost function, which we want to minimize, we're adding further restrictions on these parameters\n",
    " \n",
    " - Notice how $\\theta_{0}$ is not in the formula for P as the limits start from i=1 to i=n there, as we're not penalizing our Intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br><br><br>\n",
    "### Lasso - Implementation \n",
    "\n",
    "- The  α  parameter in Lasso tunes the strength of the penalty, and should be determined via cross-validation. (What is cross-validation? Later below)\n",
    "\n",
    "- Let's use Lasso on our model and see how it performs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-25 11:12:37--  https://raw.githubusercontent.com/Wayan123/dataset-ml/main/house_prices_multivariate.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 160215 (156K) [text/plain]\n",
      "Saving to: ‘house_prices_multivariate.csv’\n",
      "\n",
      "house_prices_multiv 100%[===================>] 156,46K  --.-KB/s    in 0,05s   \n",
      "\n",
      "2022-08-25 11:12:37 (2,86 MB/s) - ‘house_prices_multivariate.csv’ saved [160215/160215]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# link dataset: https://raw.githubusercontent.com/Wayan123/dataset-ml/main/house_prices_multivariate.csv\n",
    "\n",
    "!wget https://raw.githubusercontent.com/Wayan123/dataset-ml/main/house_prices_multivariate.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>548</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>...</td>\n",
       "      <td>460</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>434</td>\n",
       "      <td>...</td>\n",
       "      <td>608</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>...</td>\n",
       "      <td>642</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>...</td>\n",
       "      <td>836</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  YearRemodAdd  \\\n",
       "0         65.0     8450            7            5       2003          2003   \n",
       "1         80.0     9600            6            8       1976          1976   \n",
       "2         68.0    11250            7            5       2001          2002   \n",
       "3         60.0     9550            7            5       1915          1970   \n",
       "4         84.0    14260            8            5       2000          2000   \n",
       "\n",
       "   MasVnrArea  BsmtFinSF1  BsmtFinSF2  BsmtUnfSF  ...  GarageArea  WoodDeckSF  \\\n",
       "0       196.0         706           0        150  ...         548           0   \n",
       "1         0.0         978           0        284  ...         460         298   \n",
       "2       162.0         486           0        434  ...         608           0   \n",
       "3         0.0         216           0        540  ...         642           0   \n",
       "4       350.0         655           0        490  ...         836         192   \n",
       "\n",
       "   OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  \\\n",
       "0           61              0          0            0         0        0   \n",
       "1            0              0          0            0         0        0   \n",
       "2           42              0          0            0         0        0   \n",
       "3           35            272          0            0         0        0   \n",
       "4           84              0          0            0         0        0   \n",
       "\n",
       "   YrSold  SalePrice  \n",
       "0    2008     208500  \n",
       "1    2007     181500  \n",
       "2    2008     223500  \n",
       "3    2006     140000  \n",
       "4    2008     250000  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"house_prices_multivariate.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "y = data['SalePrice']\n",
    "\n",
    "# split the data with 50% in each set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=9, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7689282903544467"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "\n",
    "\n",
    "lasso_model=Lasso(alpha=140, max_iter=100000, random_state=9)\n",
    "\n",
    "# fit the model on one set of data\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model on the second set of data\n",
    "y_pred = lasso_model.predict(X_test)\n",
    "\n",
    "mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Getting the r2 score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br><br><br>\n",
    "## L2 Regularization (also called as Ridge penalisation) \n",
    "***\n",
    "<br />\n",
    "This proceeds by penalising the sum of squares (2-norms) of the model coefficients<br/>\n",
    "Here the Penalty (P) on the model will be:\n",
    "$$ P=\\alpha\\sum_{i=1}^{n}\\theta_{n}^{2}  $$\n",
    "<br/>\n",
    "And the cost function would be:<br/>\n",
    "$$ J(\\theta)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}x^{(i)}-y^{(i)})^2 + \\alpha\\sum_{n=1}^{N} \\theta_{n}^{2} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br><br><br>\n",
    "### Understanding L2\n",
    "\n",
    "- The L2 regularization will force the parameters to be relatively small, the bigger the penalization, the smaller (and the more robust to overfitting) the coefficients are\n",
    "- Here we are considering every feature but we are penalizing the coefficients based on how significant the feature is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here,  α is a hyper-parameter that controls the strength of the penalty.\n",
    "The  α parameter controls complexity of the resulting model.\n",
    "- In the limit  α→0 , we recover the standard linear regression result.\n",
    "- In the limit  α→∞ , all model responses will be suppressed.\n",
    "\n",
    "Ridge regression can be computed very efficiently at hardly any computational cost than the original linear regression model.\n",
    "This type of penalized model is built into Scikit-Learn with the Ridge estimator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "### Ridge - Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7685337209181896"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model=Ridge(alpha=0.00001, max_iter=100000, random_state=9)\n",
    "\n",
    "# fit the model on one set of data\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model on the second set of data\n",
    "y_pred = ridge_model.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)\n",
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<br><br><br>\n",
    "## L1 vs L2 Regularization\n",
    "***\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/bluedataconsulting/AIMasteryProgram/main/Lab_Exercises/Module4/images/L1_L2.png\" width=500 height=250><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br><br><br>\n",
    "## Ridge or Lasso ?\n",
    "***\n",
    "\n",
    "\n",
    "* Let's say we have a large dataset which has 10,000 features. \n",
    "* And some of the independent features are correlated with other independent features. \n",
    "\n",
    "**Which one would suit better, Rigde or Lasso?**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* If we apply **ridge regression** to it, it will retain all of the features but will **shrink the coefficients**. Still the problem is that model will remain complex as there are 10,000 features, thus may lead to poor model performance.\n",
    "\n",
    "* If we apply **lasso regression** to this problem, the main problem will be when we have correlated variables, it would retain only one variable and set other correlated variables to zero. \n",
    "* That will possibly lead to some loss of information resulting in lower accuracy in our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank You"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf26p38gpu-riset')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f25b7125f6d016f1a2196ebcb79236770b2f4743b020ee153e77d05bf60e445"
   }
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
