{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake (Q-Learning)\n",
    "\n",
    "Contoh Deloyment untuk Domain Reinforcement Learning (RL) <br>\n",
    "Orbit Future Academy - AI Mastery - KM Batch 3 <br>\n",
    "Tim Deployment dan Tim RL <br>\n",
    "2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules dan Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym==0.17.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rwRCWL3qNcwj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peta (ada 5 peta)\n",
    "peta = [\n",
    "    ['SFFF','FHFH','FFFH','HFFG'],\n",
    "    ['SFFF','FFHF','HFFF','HFFG'],\n",
    "    ['SHFF','FHFH','FFFH','HHFG'],\n",
    "    ['SFFF','HHFF','FFFF','HFFG'],\n",
    "    ['SFFH','FFFH','HFFH','HHFG']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-v9FRSOlP2Bh"
   },
   "outputs": [],
   "source": [
    "# Load Environment\n",
    "env = gym.make(\"FrozenLake-v0\",is_slippery=False, desc=peta[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banyak State  : 16\n",
      "Banyak Action : 4\n"
     ]
    }
   ],
   "source": [
    "n_observations = env.observation_space.n\n",
    "n_actions      = env.action_space.n\n",
    "\n",
    "print('Banyak State  : ' + str(n_observations))\n",
    "print('Banyak Action : ' + str(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = [\"KIRI\",\"BAWAH\",\"KANAN\",\"ATAS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXwqtjmiUu9t"
   },
   "source": [
    "## Exploring the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daftar Gerakan/Actions : <br>\n",
    "\n",
    "‚óÄÔ∏è KIRI  = 0 <br>\n",
    "üîΩ BAWAH = 1 <br>\n",
    "‚ñ∂Ô∏è KANAN = 2 <br>\n",
    "üîº ATAS  = 3 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State : 1\n",
      "Reward    : 0.0\n",
      "Done      : False\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Langkah 1 (ke Kanan)\n",
    "new_state, reward, done, info = env.step(2)\n",
    "\n",
    "print('New State : ' + str(new_state))\n",
    "print('Reward    : ' + str(reward))\n",
    "print('Done      : ' + str(done))\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State : 2\n",
      "Reward    : 0.0\n",
      "Done      : False\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Langkah 2 (ke Kanan)\n",
    "new_state, reward, done, info = env.step(2)\n",
    "\n",
    "print('New State : ' + str(new_state))\n",
    "print('Reward    : ' + str(reward))\n",
    "print('Done      : ' + str(done))\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State : 6\n",
      "Reward    : 0.0\n",
      "Done      : False\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Langkah 3 (ke Bawah)\n",
    "new_state, reward, done, info = env.step(1)\n",
    "\n",
    "print('New State : ' + str(new_state))\n",
    "print('Reward    : ' + str(reward))\n",
    "print('Done      : ' + str(done))\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State : 10\n",
      "Reward    : 0.0\n",
      "Done      : False\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Langkah 4 (ke Bawah)\n",
    "new_state, reward, done, info = env.step(1)\n",
    "\n",
    "print('New State : ' + str(new_state))\n",
    "print('Reward    : ' + str(reward))\n",
    "print('Done      : ' + str(done))\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State : 14\n",
      "Reward    : 0.0\n",
      "Done      : False\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n"
     ]
    }
   ],
   "source": [
    "# Langkah 5 (ke Bawah)\n",
    "new_state, reward, done, info = env.step(1)\n",
    "\n",
    "print('New State : ' + str(new_state))\n",
    "print('Reward    : ' + str(reward))\n",
    "print('Done      : ' + str(done))\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State : 15\n",
      "Reward    : 1.0\n",
      "Done      : True\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Langkah 6 (ke Kanan)\n",
    "new_state, reward, done, info = env.step(2)\n",
    "\n",
    "print('New State : ' + str(new_state))\n",
    "print('Reward    : ' + str(reward))\n",
    "print('Done      : ' + str(done))\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model/Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env):\n",
    "    # banyak episode yang akan dijalankan\n",
    "    n_episodes = 10000\n",
    "\n",
    "    # iterasi maksimum per episode\n",
    "    max_iter_episode = 100\n",
    "\n",
    "    # inisialisasi probabilitas eksplorasi\n",
    "    exploration_proba = 1\n",
    "\n",
    "    # penurunan eksplorasi secara eksponensial\n",
    "    exploration_decreasing_decay = 0.001\n",
    "\n",
    "    # probabilitas eksplorasi minimum\n",
    "    min_exploration_proba = 0.01\n",
    "\n",
    "    # faktor diskon\n",
    "    gamma = 0.99\n",
    "\n",
    "    # learning rate\n",
    "    lr = 0.1\n",
    "    \n",
    "    # Inisialisasi Q-table \n",
    "    Q_table = np.zeros((n_observations,n_actions))\n",
    "    \n",
    "    total_rewards_episode = list()\n",
    "    rewards_per_episode=[]\n",
    "    \n",
    "    for e in range(n_episodes):\n",
    "        current_state = env.reset()\n",
    "        done = False\n",
    "        total_episode_reward = 0\n",
    "\n",
    "        for i in range(max_iter_episode):      \n",
    "            if np.random.uniform(0,1) < exploration_proba:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q_table[current_state,:])\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            Q_table[current_state, action] = (1-lr) * Q_table[current_state, action] +lr*(reward + gamma*max(Q_table[next_state,:]))\n",
    "            total_episode_reward = total_episode_reward + reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            current_state = next_state\n",
    "\n",
    "        exploration_proba = max(min_exploration_proba, np.exp(-exploration_decreasing_decay*e))\n",
    "        rewards_per_episode.append(total_episode_reward)\n",
    "    \n",
    "    print(\"Rata-Rata Reward per 1000 Episode\")\n",
    "    for i in range(10):\n",
    "        print((i+1)*1000, \" : Rata-Rata Reward: \", np.mean(rewards_per_episode[1000*i:1000*(i+1)])) \n",
    "    \n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peta : \n",
      "['SFFF', 'FHFH', 'FFFH', 'HFFG']\n",
      "\n",
      "Rata-Rata Reward per 1000 Episode\n",
      "1000  : Rata-Rata Reward:  0.277\n",
      "2000  : Rata-Rata Reward:  0.749\n",
      "3000  : Rata-Rata Reward:  0.905\n",
      "4000  : Rata-Rata Reward:  0.967\n",
      "5000  : Rata-Rata Reward:  0.989\n",
      "6000  : Rata-Rata Reward:  0.993\n",
      "7000  : Rata-Rata Reward:  0.989\n",
      "8000  : Rata-Rata Reward:  0.989\n",
      "9000  : Rata-Rata Reward:  0.991\n",
      "10000  : Rata-Rata Reward:  0.989\n",
      "\n",
      "Peta : \n",
      "['SFFF', 'FFHF', 'HFFF', 'HFFG']\n",
      "\n",
      "Rata-Rata Reward per 1000 Episode\n",
      "1000  : Rata-Rata Reward:  0.367\n",
      "2000  : Rata-Rata Reward:  0.771\n",
      "3000  : Rata-Rata Reward:  0.93\n",
      "4000  : Rata-Rata Reward:  0.977\n",
      "5000  : Rata-Rata Reward:  0.987\n",
      "6000  : Rata-Rata Reward:  0.987\n",
      "7000  : Rata-Rata Reward:  0.994\n",
      "8000  : Rata-Rata Reward:  0.992\n",
      "9000  : Rata-Rata Reward:  0.993\n",
      "10000  : Rata-Rata Reward:  0.992\n",
      "\n",
      "Peta : \n",
      "['SHFF', 'FHFH', 'FFFH', 'HHFG']\n",
      "\n",
      "Rata-Rata Reward per 1000 Episode\n",
      "1000  : Rata-Rata Reward:  0.149\n",
      "2000  : Rata-Rata Reward:  0.596\n",
      "3000  : Rata-Rata Reward:  0.852\n",
      "4000  : Rata-Rata Reward:  0.939\n",
      "5000  : Rata-Rata Reward:  0.975\n",
      "6000  : Rata-Rata Reward:  0.986\n",
      "7000  : Rata-Rata Reward:  0.989\n",
      "8000  : Rata-Rata Reward:  0.982\n",
      "9000  : Rata-Rata Reward:  0.984\n",
      "10000  : Rata-Rata Reward:  0.982\n",
      "\n",
      "Peta : \n",
      "['SFFF', 'HHFF', 'FFFF', 'HFFG']\n",
      "\n",
      "Rata-Rata Reward per 1000 Episode\n",
      "1000  : Rata-Rata Reward:  0.437\n",
      "2000  : Rata-Rata Reward:  0.85\n",
      "3000  : Rata-Rata Reward:  0.97\n",
      "4000  : Rata-Rata Reward:  0.982\n",
      "5000  : Rata-Rata Reward:  0.998\n",
      "6000  : Rata-Rata Reward:  0.995\n",
      "7000  : Rata-Rata Reward:  0.997\n",
      "8000  : Rata-Rata Reward:  0.993\n",
      "9000  : Rata-Rata Reward:  0.997\n",
      "10000  : Rata-Rata Reward:  0.994\n",
      "\n",
      "Peta : \n",
      "['SFFH', 'FFFH', 'HFFH', 'HHFG']\n",
      "\n",
      "Rata-Rata Reward per 1000 Episode\n",
      "1000  : Rata-Rata Reward:  0.291\n",
      "2000  : Rata-Rata Reward:  0.762\n",
      "3000  : Rata-Rata Reward:  0.897\n",
      "4000  : Rata-Rata Reward:  0.969\n",
      "5000  : Rata-Rata Reward:  0.984\n",
      "6000  : Rata-Rata Reward:  0.995\n",
      "7000  : Rata-Rata Reward:  0.986\n",
      "8000  : Rata-Rata Reward:  0.987\n",
      "9000  : Rata-Rata Reward:  0.987\n",
      "10000  : Rata-Rata Reward:  0.993\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q_table_all = []\n",
    "\n",
    "for peta_env in peta:\n",
    "    # Load Environment untuk setiap peta\n",
    "    env = gym.make(\"FrozenLake-v0\",is_slippery=False, desc=peta_env)\n",
    "    env.reset()\n",
    "    \n",
    "    print('Peta : ')\n",
    "    print(peta_env)\n",
    "    print()\n",
    "    \n",
    "    # Melatih Agent\n",
    "    Q_table = train_agent(env)\n",
    "    \n",
    "    # Menyimpan Q_table\n",
    "    Q_table_all.append(Q_table)\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memainkan Agent yang Telah Dilatih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_peta = 0 # silahkan pilih peta\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\",is_slippery=False, desc=peta[index_peta])\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langkah ke  : 1\n",
      "Best Action : BAWAH\n",
      "New State   : 4\n",
      "Reward      : 0.0\n",
      "Done        : False\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Langkah ke  : 2\n",
      "Best Action : BAWAH\n",
      "New State   : 8\n",
      "Reward      : 0.0\n",
      "Done        : False\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "\n",
      "Langkah ke  : 3\n",
      "Best Action : KANAN\n",
      "New State   : 9\n",
      "Reward      : 0.0\n",
      "Done        : False\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "\n",
      "Langkah ke  : 4\n",
      "Best Action : KANAN\n",
      "New State   : 10\n",
      "Reward      : 0.0\n",
      "Done        : False\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "\n",
      "Langkah ke  : 5\n",
      "Best Action : BAWAH\n",
      "New State   : 14\n",
      "Reward      : 0.0\n",
      "Done        : False\n",
      "\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "\n",
      "Langkah ke  : 6\n",
      "Best Action : KANAN\n",
      "New State   : 15\n",
      "Reward      : 1.0\n",
      "Done        : True\n",
      "\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for langkah in range(1,7):\n",
    "    if langkah == 1:\n",
    "        best_action = np.argmax(Q_table_all[index_peta][0])\n",
    "    else:\n",
    "        best_action = np.argmax(Q_table_all[index_peta][current_state])\n",
    "        \n",
    "    new_state, reward, done, info = env.step(best_action)\n",
    "\n",
    "    print('Langkah ke  : ' + str(langkah))\n",
    "    print('Best Action : ' + ACTION[best_action])\n",
    "    print('New State   : ' + str(new_state))\n",
    "    print('Reward      : ' + str(reward))\n",
    "    print('Done        : ' + str(done))\n",
    "    print()\n",
    "\n",
    "    env.render()\n",
    "    \n",
    "    current_state = new_state\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Periksa Apakah Agent Dapat Menyelesaikan Semua Peta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peta   : ['SFFF', 'FHFH', 'FFFH', 'HFFG']\n",
      "Status : Agent dapat menyelesaikan peta ini\n",
      "\n",
      "Peta   : ['SFFF', 'FFHF', 'HFFF', 'HFFG']\n",
      "Status : Agent dapat menyelesaikan peta ini\n",
      "\n",
      "Peta   : ['SHFF', 'FHFH', 'FFFH', 'HHFG']\n",
      "Status : Agent dapat menyelesaikan peta ini\n",
      "\n",
      "Peta   : ['SFFF', 'HHFF', 'FFFF', 'HFFG']\n",
      "Status : Agent dapat menyelesaikan peta ini\n",
      "\n",
      "Peta   : ['SFFH', 'FFFH', 'HFFH', 'HHFG']\n",
      "Status : Agent dapat menyelesaikan peta ini\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for peta_env in peta:\n",
    "    print(\"Peta   : \" + str(peta_env))\n",
    "    \n",
    "    env = gym.make(\"FrozenLake-v0\",is_slippery=False, desc=peta_env)\n",
    "    env.reset()\n",
    "    \n",
    "    for langkah in range(1,7):\n",
    "        if langkah == 1:\n",
    "            best_action = np.argmax(Q_table_all[index_peta][0])\n",
    "        else:\n",
    "            best_action = np.argmax(Q_table_all[index_peta][current_state])\n",
    "\n",
    "        new_state, reward, done, info = env.step(best_action)\n",
    "\n",
    "        current_state = new_state\n",
    "\n",
    "    if done == True:\n",
    "        print(\"Status : Agent dapat menyelesaikan peta ini\")\n",
    "    else:\n",
    "        print(\"Status : Agent tidak dapat menyelesaikan peta ini\")\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menyimpan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Q_table_all, open('Q_table_Frozen_Lake.model', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Module12_(3.2)_Reinforcement_Learning_Lab_Exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf_env2",
   "language": "python",
   "name": "tf_env2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
