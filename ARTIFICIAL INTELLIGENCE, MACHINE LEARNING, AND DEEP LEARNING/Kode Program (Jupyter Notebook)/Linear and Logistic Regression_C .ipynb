{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4\n",
    "## Section: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2: Logistic Regression and Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "## Objective\n",
    "***\n",
    "- Approach\n",
    "- Logistic Regression\n",
    "    - Cost Function\n",
    "    - Gradient Descent\n",
    "    - Evaluation Metrics for Logistic Regression\n",
    "        - Confusion Matrix\n",
    "        - Precision and Recall\n",
    "        - F-1 score\n",
    "        - Area under ROC curve\n",
    "        - Logarithmic Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br><br><br><br>\n",
    "## Approach\n",
    "***\n",
    "- After learning basics, lets see how the Loan Prediction data set could use the same techniques\n",
    "- We want further insight to what this data set looks like and how we would go about implementing this \n",
    "- It would be smart if we split this data set into a *Training Set* and *Test Set* \n",
    "\n",
    "    - I'll leave it to you to figure out why this would be appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>\n",
    "## Logistic Regression - Cost Function and Gradient Descent\n",
    "***\n",
    "- Till now we studied the intuition behind the Sigmoid Function\n",
    "\n",
    "- We also studied how Logistic Regression works to get outputs in the range of [0,1]\n",
    "\n",
    "- We discussed the interpretation of the output too! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function \n",
    "***\n",
    " - Fit θ parameters\n",
    " - Define the optimization object for the cost function we use to fit the parameters\n",
    "     - Training set consists of **\"m\"** training examples\n",
    "         - Each example has a **n+1** length column vector\n",
    "***      \n",
    " \n",
    "<center>Training set:  $\\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\cdots,(x^{(m)},y^{(m)})\\}$<br/><br/></center>\n",
    "<center>m examples $x \\epsilon \\begin{bmatrix}x_{0}\\\\x_{1}\\\\\\cdots\\\\x_{n}\\end{bmatrix}$</center><center>$x_{0}=1,y\\epsilon \\{0,1\\}$</center> \n",
    "\n",
    " $$ h_{\\theta}(x) = \\frac{1}{1+e^{-\\theta^TX}} $$\n",
    " \n",
    " ***\n",
    "* This is the situation: \n",
    "  - Set of m training examples\n",
    "  - Each example is a feature vector which is n+1 dimensional\n",
    "  - $x_0$ = 1\n",
    "  - y ∈ {0,1}\n",
    "  - Hypothesis is based on parameters (θ)\n",
    "      - **Given the training set how to we chose/fit θ?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Our cost function for \"m\" training examples is: \n",
    "***\n",
    "  $$ J(\\theta) =- \\frac{1}{m}[\\sum_{i=1}^my^{(i)}logh_\\theta(x^{(i)})+(1-y^{(i)})log(1-h_\\theta(x^{(i)}))] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient Descent\n",
    "***\n",
    " <center>Repeat for all $\\theta_j$ simultaneously { <br> $\\theta_j := \\theta_j - \\alpha\\sum_{i=1}^m(h_{\\theta}(x^{(i)})- y^{(i)})x_{j}^{(i)}$</center>\n",
    " <center>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-08-25 11:18:23--  https://raw.githubusercontent.com/Wayan123/dataset-ml/main/loan_prediction.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... failed: Connection timed out.\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18131 (18K) [text/plain]\n",
      "Saving to: ‘loan_prediction.csv’\n",
      "\n",
      "loan_prediction.csv 100%[===================>]  17,71K  --.-KB/s    in 0,003s  \n",
      "\n",
      "2022-08-25 11:20:33 (5,16 MB/s) - ‘loan_prediction.csv’ saved [18131/18131]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download dataset from - \n",
    "# https://raw.githubusercontent.com/Wayan123/dataset-ml/main/loan_prediction.csv\n",
    "!wget https://raw.githubusercontent.com/Wayan123/dataset-ml/main/loan_prediction.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting Data\n",
    "dataframe = pd.read_csv('loan_prediction.csv')\n",
    "X = dataframe.iloc[:,:-1]\n",
    "y = dataframe.iloc[:,-1]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a logistic regression model\n",
    "logistic_regressor = LogisticRegression(max_iter=1000)\n",
    "pipeline = Pipeline(steps=[('add_poly_features', PolynomialFeatures()),\n",
    "                           ('logistic_regression', logistic_regressor)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wayan/miniconda3/envs/tf26p38gpu-riset/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br><br><br><br>\n",
    "## Evalution Metrics for Logistic Regression\n",
    "***\n",
    "\n",
    "* As we already know, we use different metrics for regression and classification\n",
    "* We know that we can use `MSE` for regression problems and `Accuracy` for classification problems\n",
    "* However, these might not be the best metrics in every situation<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evalution Metrics for Logistic Regression\n",
    "***\n",
    "\n",
    "* Following are the types of Classification Metrics :\n",
    "    * Confusion Matrix\n",
    "    * Classification Matrix\n",
    "    * F1 Score\n",
    "    * Area under ROC curve\n",
    "    * Classification Report\n",
    "    * Logarithmic Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "### Confusion Matrix\n",
    "***\n",
    "- The confusion matrix is a handy presentation of the accuracy of a model with two or more classes. Below is an example of a Confusion Matrix \n",
    "<br><br>\n",
    "\n",
    "\n",
    "| Value | Fraud  | Not Fraud |\n",
    "|---|---|---|\n",
    "| Predicted Fraud | 1 | 1 |\n",
    "| Predicted Not Fraud | 2 | 996 |\n",
    "\n",
    "\n",
    "    True Positives (TP): These are predicted yes and actually yes (Top Left)\n",
    "    True Negatives (TN): We predicted no, and actually no (Top Right) \n",
    "    False Positives (FP): We predicted yes, but actually no. (AKA \"Type I error.\") (Top Right) \n",
    "    False Negatives (FN): We predicted no, but yes. (AKA\"Type II error.\") (Bottom Left)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Confusion Matrix\n",
    "***\n",
    "* Classification accuracy is the number of correct predictions **(TN + TP)** made as a ratio of all predictions made. **(TN + TP +FN + FP)**<br><br>\n",
    "It is suitable when :\n",
    "* There are an equal number of observations in each class\n",
    "* That all predictions and prediction errors are equally important,which is often not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 14,  43],\n",
       "       [ 12, 116]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying confusion matrix on above data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Precision\n",
    "***\n",
    "\n",
    "$$Precision = \\frac {(True +ves)} {(True +ves  +  False +ves)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7295597484276729"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "precision_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recall \n",
    "***\n",
    "\n",
    "$$Recall = \\frac {(True +ves)} {(True +ves  +  False -ves)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90625"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Trade - Off: Precision Vs. Recall \n",
    "***\n",
    "- This is more of a in-class activity! \n",
    "- Think about this: What happens if we get an increased value of Precision? Do you think that would lower Recall? And vice-versa? \n",
    "\n",
    "- Think of an example! And use easy numerical calculations too. You can just use a pencil and paper, no need for code! \n",
    "\n",
    "- [**Hint**: There is a trade-off!] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### F1 Score\n",
    "***\n",
    " - To deal with this Trade-off we calculate something known as the F-1 Score: F1 score is a good approach to minimize a bias towards either the Precision or the Recall\n",
    "\n",
    " $$F1 Score = \\frac {2PR} {P + R} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "***\n",
    "F1 Score is defined as \n",
    "\n",
    "<center>$2*\\frac{precision*recall}{precision+recall}$</center>\n",
    "\n",
    "* tp = true positive\n",
    "* tn = true negative\n",
    "* fp = false positive\n",
    "* fn = false negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "***\n",
    "- Using this intuition, we want to calculate the F-1 Score to better understand the evaluation of our model\n",
    "\n",
    "- Let's see how to implement this in Python! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8083623693379791"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## code for f-1 score\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Area under ROC Curve\n",
    "***\n",
    "ROC (Receiver Operating Characteristic) Curve tells us about how good the model can distinguish between two things (e.g If a patient has a disease or no). Better models can accurately distinguish between the two. Whereas, a poor model will have difficulties in distinguishing between the two.\n",
    "Area under ROC Curve (or AUC for short) is a performance metric for binary classification problems.\n",
    "- The AUC represents a model’s ability to discriminate between positive and negative classes.\n",
    " - An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random.<br>\n",
    "**Brain Teaser**: What does area < 0.5 signify?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "***\n",
    "ROC can be broken down into sensitivity and specificity. Let's understand these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sensitivity and Specificity.\n",
    "Let us take an example of patients having a disease.\n",
    "\n",
    "In simple terms, the proportion of patients that were identified correctly to have the disease (i.e. True Positive) upon the total number of patients who actually have the disease is called as Sensitivity or Recall.\n",
    "\n",
    "\n",
    "Similarly, the proportion of patients that were identified correctly to not have the disease (i.e. True Negative) upon the total number of patients who do not have the disease is called as Specificity.\n",
    "\n",
    "\n",
    "Trade-off between Sensitivity and Specificity\n",
    "When we decrease the threshold, we get more positive values thus increasing the sensitivity. Meanwhile, this will decrease the specificity.\n",
    "\n",
    "Similarly, when we increase the threshold, we get more negative values thus increasing the specificity and decreasing sensitivity.\n",
    "\n",
    "## ROC \n",
    "ROC is nothing but a plot of sensitivity also known as True Positive Rate against (1-specficity) also known as False Positive Rate for different values of threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5759320175438596"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=1)\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Logarithmic Loss\n",
    "***\n",
    "Logarithmic loss (or logloss) is a performance metric for evaluating the predictions of probabilities of membership to a given class\n",
    "\n",
    "\n",
    "Where,\n",
    "* N is the number of samples or instances,\n",
    "* M is the number of possible labels,\n",
    "* y<sub>ij</sub> is a binary indicator of whether or not label j is the correct classification for instance i,\n",
    "* p<sub>ij</sub> is the model probability of assigning label j to instance i.\n",
    "\n",
    " $$ Logloss=- \\frac{1}{n}\\sum_{i=1}^n[y_ilogp_{i}+(1-y_{i})log(1-p_{i})] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "***\n",
    "* The scalar probability between 0 and 1 can be seen as a measure of confidence for a prediction by an algorithm.<br>\n",
    "* Predictions that are correct or incorrect are rewarded or punished proportionally to the confidence of the prediction.<br>\n",
    "* logloss nearer to 0 is better, with 0 representing a perfect logloss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.268470726538304"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf26p38gpu-riset')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f25b7125f6d016f1a2196ebcb79236770b2f4743b020ee153e77d05bf60e445"
   }
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
